import os
import numpy as np
import pandas as pd

localrules: all, combine_all_peaks, combine_all_sites, combine_peaks

refgen_path = config["refgen_path"]

chromsizes = pd.read_table(config["chrom_sizes_path"], header=None, sep='\t', index_col=0, names=['chrom', 'size'])
cassette_length = chromsizes.loc[config['cassette_name']]['size']


# Depending on the mapper, the index files will be different
if config["mapper"] == "bwa-mem":
    idx = multiext(
        refgen_path,
        ".amb",
        ".ann",
        ".bwt",
        ".pac",
        ".sa",
    )

elif config["mapper"] == "bwa-mem2":
    idx = multiext(
        refgen_path,
        ".0123",
        ".amb",
        ".ann",
        ".bwt.2bit.64",
        ".pac",
    )

elif config['mapper'] == 'bwa-meme':
    idx = multiext(
        refgen_path,
        ".0123",
        ".amb",
        ".ann",
        ".pac",
        ".pos_packed",
        ".suffixarray_uint64",
        ".suffixarray_uint64_L0_PARAMETERS",
        ".suffixarray_uint64_L1_PARAMETERS",
        ".suffixarray_uint64_L2_PARAMETERS",
    )
else:
    raise ValueError("""Unknown mapper, allowed values are:
                        'bwa-mem', 'bwa-mem2' and 'bwa-meme'""")

samples = pd.read_table(config["samples_path"], comment='#')
sample_list = np.unique(samples['name'].to_numpy())

wildcard_constraints:
    sample=f"({'|'.join([re.escape(sample) for sample in sample_list])})",
    side=('forward|reverse')

if config['use_junction_readthrough']:
    ruleorder: map_one_sided > map
    ruleorder: parse2_one_side > parse
    ruleorder: get_trans_pairs > get_side_pairs
else:
    ruleorder: map > map_one_sided
    ruleorder: parse > parse2_one_side
    ruleorder: get_side_pairs > get_trans_pairs

rule all:
    input:
        expand("../Data/Processed/pairs/{sample}_stats.yml",
        sample=sample_list),
        "../Data/Processed/peaks/all_peaks.bed",
        "../Data/Processed/insertion_sites/all_sites.bed"
        
rule bwaindex:
    input:
        genome=refgen_path,
    output:
        idx=idx,
    params:
        bwa=config['mapper'],
    threads: 8  #Only affects bwa-meme
    log:
        f"logs/bwa-memx_index/{os.path.basename(refgen_path).rstrip('.fasta')}.log",
    cache: True
    wrapper:
        "v3.3.3/bio/bwa-memx/index"

rule merge_fastq:
    input:
        lambda wildcards: samples[samples['name']==wildcards.sample][f'fastq{wildcards.read}'],
    output:
        pipe("../Data/Processed/fastq/{sample}.{read}.fastq.gz"),
    shell:
        """
        cat {input} > {output}
        """

rule map:
    input:
        reads=lambda wildcards: [f"../Data/Processed/fastq/{wildcards.sample}.R1.fastq.gz",
                                 f"../Data/Processed/fastq/{wildcards.sample}.R2.fastq.gz"],
        reference=refgen_path,
        idx=idx,
    params:
        bwa=config['mapper'],
        sort="none",
        dedup="none",
        extra='-SP -T 20' # Lower minimal alignment score for bwa-mem to increase sensitivity for short reads and short alignments
    threads: 6
    output:
        "../Data/Processed/bams/{sample}.bam",
    log:
        "logs/bwa_memx/{sample}.log",
    benchmark:
        "benchmarks/bwa_memx/{sample}.tsv"
    wrapper:
        "v3.3.3/bio/bwa-memx/mem"

use rule map as map_one_sided with:
    input:
        reads="../Data/Processed/fastq/{sample}.R2.fastq.gz",
        reference=refgen_path,
        idx=idx

# rule parse_sort_dedup:
#     input:
#         "../Data/Processed/bams/{sample}.bam",
#     output:
#         "../Data/Processed/pairs/{sample}.pairs.gz"
#     conda:
#         "envs/pairtools_env.yaml",
#     log:
#         "logs/parse/{sample}.log",
#     benchmark:
#         "benchmarks/parse/{sample}.tsv"
#     threads: 16
#     params:
#         chrom_sizes_path=config['chrom_sizes_path'],
#         cassette_name=config['cassette_name'],
#     shell:
#         """
#         pairtools parse -c {params.chrom_sizes_path} --drop-seq --drop-sam --no-flip \
#         --add-columns mapq,pos5,pos3 --report-alignment-end 5 {input} \
#         | pairtools sort --nproc {threads} \
#         | pairtools dedup --backend cython --max-mismatch 0 -o {output}
#         """

rule parse:
    input:
        "../Data/Processed/bams/{sample}.bam",
    output:
        pairs="../Data/Processed/pairs/{sample}_parsed.pairs",
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/parse/{sample}.log",
    benchmark:
        "benchmarks/parse/{sample}.tsv"
    threads: 1
    params:
        chrom_sizes_path=config['chrom_sizes_path'],
        cassette_name=config['cassette_name'],
    shell:
        """
        pairtools parse -c {params.chrom_sizes_path} --drop-sam --no-flip \
        --min-mapq 30 \
        --walks-policy 5any \
        --add-columns mapq,pos5,pos3 --report-alignment-end 5 \
        --output {output.pairs} \
        {input} \
        >{log[0]} 2>&1
        """

rule stats:
    input:
        pairs="../Data/Processed/pairs/{sample}_parsed.pairs",
    output:
        stats="../Data/Processed/pairs/{sample}_stats.yml"
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/stats/{sample}.log",
    benchmark:
        "benchmarks/stats/{sample}.tsv"
    threads: 1
    shell:
        """
        pairtools stats --yaml {input.pairs} -o {output.stats} \
        >{log[0]} 2>&1
        """

rule parse2_one_side:
    input:
        "../Data/Processed/bams/{sample}.bam",
    output:
        pairs="../Data/Processed/pairs/{sample}_parsed.pairs",
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/parse2_one_side/{sample}.log",
    benchmark:
        "benchmarks/parse2_one_side/{sample}.tsv"
    threads: 1
    params:
        chrom_sizes_path=config['chrom_sizes_path'],
        cassette_name=config['cassette_name'],
    shell:
        """
        pairtools parse2 -c {params.chrom_sizes_path} --drop-sam --no-flip \
        --min-mapq 30 \
        --add-columns mapq,pos5,pos3 \
        --single-end \
        --report-position walk \
        --output {output.pairs} \
        {input} \
        >{log[0]} 2>&1
        """

rule sort:
    input:
        "../Data/Processed/pairs/{sample}_parsed.pairs",
    output:
        "../Data/Processed/pairs/{sample}_sorted.pairs"
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/sort/{sample}.log",
    benchmark:
        "benchmarks/sort/{sample}.tsv"
    threads: 6
    shell:
        """
        pairtools sort --nproc {threads} -o {output} {input} \
        >{log[0]} 2>&1
        """

rule dedup:
    input:
        "../Data/Processed/pairs/{sample}_sorted.pairs",
    output:
        "../Data/Processed/pairs/{sample}_nodups.pairs"
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/dedup/{sample}.log",
    benchmark:
        "benchmarks/dedup/{sample}.tsv"
    threads: 1
    shell:
        """
        pairtools dedup --backend cython --max-mismatch 0 -o {output} {input} \
        >{log[0]} 2>&1
        """

rule get_trans_pairs:
    input:
        lambda wildcards: "../Data/Processed/pairs/{sample}_nodups.pairs" if config.get('dedup', True) else "../Data/Processed/pairs/{sample}_sorted.pairs",
    output:
        "../Data/Processed/pairs/{sample}_{side}.pairs"
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/get_trans_pairs/{sample}_{side}.log",
    benchmark:
        "benchmarks/get_trans_pairs/{sample}_{side}.tsv"
    threads: 1
    params:
        selection_chrom = config['cassette_name'],
        selection_start = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']-1,
        selection_end = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']+1,
        selection_3prime = lambda wildcards: cassette_length if wildcards.side == 'forward' else 1
    shell:
        """
        pairtools select -t pos51 int -t pos31 int \
                         '(pair_type.upper()=="UR" or \
                           pair_type.upper()=="UU" or \
                           pair_type.upper()=="RU") and \
                          chrom1!=chrom2 and \
                          chrom1=="{params.selection_chrom}" and \
                          pos51>={params.selection_start} and \
                          pos51<={params.selection_end} and \
                          pos31=={params.selection_3prime}' \
        -o {output} {input} \
        >{log[0]} 2>&1
        """

rule get_side_pairs:
    input:
        lambda wildcards: "../Data/Processed/pairs/{sample}_nodups.pairs" if config.get('dedup', True) else "../Data/Processed/pairs/{sample}_sorted.pairs",
    output:
        "../Data/Processed/pairs/{sample}_{side}.pairs",
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/get_side_pairs/{sample}_{side}.log",
    benchmark:
        "benchmarks/get_side_pairs/{sample}_{side}.tsv"
    threads: 1
    params:
        selection_chrom = config['cassette_name'],
        selection_start = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']-10,
        selection_end = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']+10,
    shell:
        """
        pairtools select '(\
                          (pair_type.upper()=="UR" or \
                           pair_type.upper()=="UU" or \
                           pair_type.upper()=="RU") and \
                          (chrom2=="{params.selection_chrom}") and \
                          (pos2>{params.selection_start}) and \
                          (pos2<{params.selection_end}))' \
        -o {output} {input} \
        >{log[0]} 2>&1
        """

rule coverage:
    input:
        pairs="../Data/Processed/pairs/{sample}_{side}.pairs",
        script='workflow/scripts/coverage.py'
    output:
        bg="../Data/Processed/coverage/{sample}_{side}_coverage.bedgraph",
        bw="../Data/Processed/coverage/{sample}_{side}_coverage.bw",
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/coverage/{sample}_{side}.log",
    benchmark:
        "benchmarks/coverage/{sample}_{side}.tsv"
    threads: 1
    params:
        side=2 if config['use_junction_readthrough'] else 1
    shell:
        """
        python3 {input.script} -i {input.pairs} --side {params.side} -o {output.bg} \
        -t {threads} --output-bigwig {output.bw} \
        >{log[0]} 2>&1
        """

rule find_peaks:
    input:
        coverage="../Data/Processed/coverage/{sample}_{side}_coverage.bedgraph",
        script='workflow/scripts/find_peaks.py'
    output:
        "../Data/Processed/peaks/{sample}_{side}.bed"
    conda:
        "envs/peaks.yaml",
    log:
        "logs/find_peaks/{sample}_{side}.log",
    benchmark:
        "benchmarks/find_peaks/{sample}_{side}.tsv"
    threads: 1
    params:
        cluster_arg = lambda wildcards, input: "--no-cluster" if config['use_junction_readthrough'] else "--cluster",
        auto_li_arg = lambda wildcards, input: "--auto-li" if config['use_junction_readthrough'] else "--no-auto-li",
        min_peak_width=config['min_peak_width'],
        min_peak_reads=config['min_peak_reads'],
        min_peak_frac=config['min_peak_frac']
    shell:
        """
        python3 {input.script} -i {input.coverage} -o {output} \
        {params.cluster_arg} {params.auto_li_arg} \
        --min-peak-width {params.min_peak_width} --min-peak-frac {params.min_peak_frac} \
        --min-peak-reads {params.min_peak_reads} \
        >{log[0]} 2>&1
        """

rule combine_peaks:
    input:
        peaks_fwd="../Data/Processed/peaks/{sample}_forward.bed",
        peaks_rev="../Data/Processed/peaks/{sample}_reverse.bed",
        blacklist=config.get('blacklist', []),
        script='workflow/scripts/combine_peaks.py'
    output:
        "../Data/Processed/peaks/{sample}_peaks.bed",
    conda:
        "envs/peaks.yaml",
    log:
        "logs/combine_peaks/{sample}.log",
    benchmark:
        "benchmarks/combine_peaks/{sample}.tsv"
    threads: 1
    params:
        blacklist_arg = lambda wildcards, input: f"--blacklist {input.blacklist}" if input.blacklist else "",
    shell:
        """
        python3 {input.script} --fwd {input.peaks_fwd} --rev {input.peaks_rev} \
        {params.blacklist_arg} \
        --sample-name {wildcards.sample} -o {output} \
        >{log[0]} 2>&1
        """

rule combine_all_peaks:
    input:
        peaks=expand("../Data/Processed/peaks/{sample}_peaks.bed",
                            sample=sample_list)
    output:
        "../Data/Processed/peaks/all_peaks.bed",
    conda:
        "envs/peaks.yaml",
    log:
        "logs/combine_all_peaks/log.log",
    benchmark:
        "benchmarks/combine_all_peaks/benchmark.tsv"
    threads: 1
    shell:
        """
        cat {input.peaks} | cut -f1,2,3,4,6,7 | sort -k4,4V -k1,1V -k2,2n -k3,3n -k5,5 > {output}
        """

rule find_insertion_sites:
    input:
        peaks="../Data/Processed/peaks/all_peaks.bed",
        script='workflow/scripts/find_insertion_sites.py' #lambda wildcards: 'workflow/scripts/find_insertion_sites_readthrough.py' if config['use_junction_readthrough'] else 'workflow/scripts/find_insertion_sites.py'
    output:
        "../Data/Processed/insertion_sites/all_sites.bed",
    conda:
        "envs/peaks.yaml",
    log:
        "logs/find_insertion_sites/all.log",
    benchmark:
        "benchmarks/find_insertion_sites/all.tsv"
    threads: 1
    params:
        refgen_path=refgen_path,
        insertion_seq=config['insertion_seq']
    shell:
        """
        python3 {input.script} --peaks {input.peaks} \
        --genome {params.refgen_path} --insertion-seq {params.insertion_seq} \
        -o {output} \
        >{log[0]} 2>&1
        """

# rule combine_all_sites:
#     input:
#         sites=expand("../Data/Processed/insertion_sites/{sample}.bed",
#                             sample=sample_list)
#     output:
#         "../Data/Processed/insertion_sites/all_sites.bed",
#     conda:
#         "envs/peaks.yaml",
#     log:
#         "logs/combine_all_sites/log.log",
#     benchmark:
#         "benchmarks/combine_all_sites/benchmark.tsv"
#     threads: 1
#     shell:
#         """
#         cat {input.sites} | cut -f1,2,3,4,6,7 | sort -k4,4V -k1,1V -k2,2n -k3,3n -k5,5 > {output}
#         """