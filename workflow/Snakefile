import os
import numpy as np
import pandas as pd

configfile: "../config/example_config.yaml"

localrules: all, combine_all_peaks, combine_peaks

refgen_path = config["refgen_path"]

chromsizes = pd.read_table(config["chrom_sizes_path"], header=None, sep='\t', index_col=0, names=['chrom', 'size'])
cassette_length = chromsizes.loc[config['cassette_name']]['size']

output_dir = config["output_dir"]

# Depending on the mapper, the index files will be different
if config["mapper"] == "bwa-mem":
    idx = multiext(
        refgen_path,
        ".amb",
        ".ann",
        ".bwt",
        ".pac",
        ".sa",
    )

elif config["mapper"] == "bwa-mem2":
    idx = multiext(
        refgen_path,
        ".0123",
        ".amb",
        ".ann",
        ".bwt.2bit.64",
        ".pac",
    )

elif config['mapper'] == 'bwa-meme':
    idx = multiext(
        refgen_path,
        ".0123",
        ".amb",
        ".ann",
        ".pac",
        ".pos_packed",
        ".suffixarray_uint64",
        ".suffixarray_uint64_L0_PARAMETERS",
        ".suffixarray_uint64_L1_PARAMETERS",
        ".suffixarray_uint64_L2_PARAMETERS",
    )
else:
    raise ValueError("""Unknown mapper, allowed values are:
                        'bwa-mem', 'bwa-mem2' and 'bwa-meme'""")

samples = pd.read_table(config["samples_path"], comment='#')
sample_list = np.unique(samples['name'].to_numpy())

wildcard_constraints:
    sample="({'|'.join([re.escape(sample) for sample in sample_list])})",
    side=('forward|reverse'),

# if config['use_only_read_junctions']:
#     # ruleorder: bwamap_one_sided > bwamap
#     ruleorder: parse2 > parse
#     ruleorder: get_trans_pairs > get_side_pairs
# else:
#     # ruleorder: bwamap > bwamap_one_sided
#     ruleorder: parse > parse2
#     ruleorder: get_side_pairs > get_trans_pairs

rule all:
    input:
        expand(os.path.join(output_dir, "pairs/{sample}_stats.yml"),
        sample=sample_list),
        expand(os.path.join(output_dir, "pairs/{sample}_{side}_stats.yml"),
        sample=sample_list, side=['forward', 'reverse']),
        os.path.join(output_dir, "peaks/all_peaks.bed"),
        os.path.join(output_dir, "insertion_sites/all_sites.bed"),
        expand(os.path.join(output_dir, "coverage/{sample}_{side}_coverage_for_ucsc.bedgraph"),
        sample=sample_list, side=['forward', 'reverse']),
        
rule bwaindex:
    input:
        refgen_path,
    output:
        idx,
    params:
        bwa=config['mapper'],
    priority: 100
    threads: 8  #Only affects bwa-meme
    log:
        "logs/bwa-memx_index/{os.path.basename(refgen_path).rstrip('.fasta').rstrip('.fa')}.log",
    wrapper:
        "v3.8.0/bio/bwa-memx/index"

rule merge_fastq:
    input:
        lambda wildcards: samples[samples['name']==wildcards.sample][f'fastq{wildcards.read}'],
    output:
        pipe(os.path.join(output_dir, "fastq/{sample}.{read}.fastq.gz")),
    shell:
        """
        cat {input} > {output}
        """

rule bwamap:
    input:
        reads=lambda wildcards: [os.path.join(output_dir, "fastq/{wildcards.sample}.R1.fastq.gz"),
                                 os.path.join(output_dir, "fastq/{wildcards.sample}.R2.fastq.gz")],
        reference=refgen_path,
        idx=idx,
    params:
        bwa=config['mapper'],
        sort="none",
        dedup="none",
        extra='-SP -T 20' # Lower minimal alignment score for bwa-mem to increase sensitivity for short reads and short alignments
    threads: 6
    output:
        os.path.join(output_dir, "bams/{sample}.bam"),
    log:
        "logs/bwa_memx/{sample}.log",
    benchmark:
        "benchmarks/bwa_memx/{sample}.tsv"
    wrapper:
        "v3.3.3/bio/bwa-memx/mem"


# rule parse_sort_dedup:
#     input:
#         os.path.join(output_dir, "bams/{sample}.bam",
#     output:
#         os.path.join(output_dir, "pairs/{sample}.pairs.gz"
#     conda:
#         "envs/pairtools_env.yaml",
#     log:
#         "logs/parse/{sample}.log",
#     benchmark:
#         "benchmarks/parse/{sample}.tsv"
#     threads: 16
#     shell:
#         """
#         pairtools parse -c {params.chrom_sizes_path} --drop-seq --drop-sam --no-flip \
#         --add-columns mapq,pos5,pos3 --report-alignment-end 5 {input} \
#         | pairtools sort --nproc {threads} \
#         | pairtools dedup --backend cython --max-mismatch 0 -o {output}
#         """

# rule parse:
#     input:
#         bam=os.path.join(output_dir, "bams/{sample}.bam",
#         chromsizes=config['chrom_sizes_path']
#     output:
#         pairs=os.path.join(output_dir, "pairs/{sample}_parsed.pairs",
#     conda:
#         "envs/pairtools_env.yaml",
#     log:
#         "logs/parse/{sample}.log",
#     benchmark:
#         "benchmarks/parse/{sample}.tsv"
#     threads: 1
#     shell:
#         """
#         pairtools parse -c {input.chromsizes} --drop-sam --no-flip \
#         --min-mapq 30 \
#         --walks-policy all \
#         --add-columns mapq,pos5,pos3,read_len --report-alignment-end 5 \
#         --output {output.pairs} \
#         {input.bam} \
#         >{log[0]} 2>&1
#         """

rule parse2:
    input:
        bam=os.path.join(output_dir, "bams/{sample}.bam"),
        chromsizes=config['chrom_sizes_path']
    output:
        pairs=pipe(os.path.join(output_dir, "pairs/{sample}_parsed.pairs")),
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/parse2/{sample}.log",
    benchmark:
        "benchmarks/parse2/{sample}.tsv"
    threads: 1
    shell:
        """
        pairtools parse2 -c {input.chromsizes} --drop-sam --no-flip \
        --min-mapq 30 \
        --add-columns mapq,pos5,pos3,read_len \
        --add-pair-index \
        --report-position read \
        --report-orientation pair \
        --output {output.pairs} \
        {input.bam} \
        >{log[0]} 2>&1
        """

rule sort:
    input:
        os.path.join(output_dir, "pairs/{sample}_parsed.pairs"),
    output:
        os.path.join(output_dir, "pairs/{sample}_sorted.pairs")
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/sort/{sample}.log",
    benchmark:
        "benchmarks/sort/{sample}.tsv"
    threads: 6
    shell:
        """
        pairtools sort --nproc {threads} -o {output} {input} \
        >{log[0]} 2>&1
        """

rule stats:
    input:
        pairs=os.path.join(output_dir, "pairs/{sample}_sorted.pairs"),
    output:
        stats=os.path.join(output_dir, "pairs/{sample}_stats.yml")
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/stats/{sample}.log",
    benchmark:
        "benchmarks/stats/{sample}.tsv"
    threads: 1
    shell:
        """
        pairtools stats --yaml {input.pairs} -o {output.stats} \
        >{log[0]} 2>&1
        """

rule dedup:
    input:
        os.path.join(output_dir, "pairs/{sample}_sorted.pairs"),
    output:
        os.path.join(output_dir, "pairs/{sample}_nodups.pairs")
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/dedup/{sample}.log",
    benchmark:
        "benchmarks/dedup/{sample}.tsv"
    threads: 1
    shell:
        """
        pairtools dedup --backend cython --max-mismatch 0 -o {output} {input} \
        >{log[0]} 2>&1
        """

rule get_trans_side_pairs:
    input:
        lambda wildcards: os.path.join(output_dir, "pairs/{sample}_nodups.pairs") if config.get('dedup', True) else os.path.join(output_dir, "pairs/{sample}_sorted.pairs")
    output:
        os.path.join(output_dir, "pairs/{sample}_{side}.pairs")
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/get_trans_pairs/{sample}_{side}.log",
    benchmark:
        "benchmarks/get_trans_pairs/{sample}_{side}.tsv"
    threads: 1
    params:
        selection_chrom = config['cassette_name'],
        walk_pair_type_arg = 'walk_pair_type.isin(["R1", "R2", "R1&2"]) and' if config['use_only_read_junctions'] else '',
        selection_3prime = lambda wildcards: "min({cassette_length}, {config['forward_ITR_primer_position']}+read_len1)" if wildcards.side == 'forward' else "max(0, {config['reverse_ITR_primer_position']}-read_len2)"
    shell:
        """
        pairtools select -t pos51 int -t pos31 int -t pos52 int -t pos32 int \
                         -t read_len1 int -t read_len2 int \
                         '(pair_type.upper()=="UR" or \
                           pair_type.upper()=="UU" or \
                           pair_type.upper()=="RU") and \
                           chrom1!=chrom2 and \
                           {params.walk_pair_type_arg} \
                          (chrom2=="{params.selection_chrom}" and \
                            abs(pos32-{params.selection_3prime})<=2)' \
                        -o {output} {input} \
        >{log[0]} 2>&1
        """

use rule stats as stats_side with:
    input:
        pairs=os.path.join(output_dir, "pairs/{sample}_{side}.pairs"),
    log:
        "logs/stats/{sample}_{side}.log",
    benchmark:
        "benchmarks/stats/{sample}_{side}.tsv"
    output:
        stats=os.path.join(output_dir, "pairs/{sample}_{side}_stats.yml")

# rule get_readthrough_trans_pairs:
#     input:
#         lambda wildcards: os.path.join(output_dir, "pairs/{sample}_nodups.pairs" if config.get('dedup', True) else os.path.join(output_dir, "pairs/{sample}_sorted.pairs",
#     output:
#         os.path.join(output_dir, "pairs/{sample}_{side}.pairs"
#     conda:
#         "envs/pairtools_env.yaml",
#     log:
#         "logs/get_trans_pairs/{sample}_{side}.log",
#     benchmark:
#         "benchmarks/get_trans_pairs/{sample}_{side}.tsv"
#     threads: 1
#     params:
#         selection_chrom = config['cassette_name'],
#         # selection_start = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position'] if wildcards.side == 'forward' else 0,
#         # selection_end = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']+1,
#         selection_3prime = lambda wildcards: cassette_length if wildcards.side == 'forward' else 1
#     shell:
#         """
#         pairtools select -t pos51 int -t pos31 int -t pos52 int -t pos32 int \
#                          '(pair_type.upper()=="UR" or \
#                            pair_type.upper()=="UU" or \
#                            pair_type.upper()=="RU") and \
#                           chrom1!=chrom2 and \
#                           walk_pair_type.isin(["R1", "R2", "R1&2"]) and \
#                             ( \
#                                 (chrom1=="{params.selection_chrom}" and \
#                                 pos31=={params.selection_3prime}) \

#                                 or \

#                                 (chrom2=="{params.selection_chrom}" and \
#                                  pos32=={params.selection_3prime}) \
#                             )' \
#         -o {output} {input} \
#         >{log[0]} 2>&1
#         """

# rule get_side_pairs:
#     input:
#         lambda wildcards: os.path.join(output_dir, "pairs/{sample}_nodups.pairs" if config.get('dedup', True) else os.path.join(output_dir, "pairs/{sample}_sorted.pairs",
#     output:
#         os.path.join(output_dir, "pairs/{sample}_{side}.pairs",
#     conda:
#         "envs/pairtools_env.yaml",
#     log:
#         "logs/get_side_pairs/{sample}_{side}.log",
#     benchmark:
#         "benchmarks/get_side_pairs/{sample}_{side}.tsv"
#     threads: 1
#     params:
#         selection_chrom = config['cassette_name'],
#         selection_start = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']-10,
#         selection_end = lambda wildcards: config[f'{wildcards.side}_ITR_primer_position']+10,
#     shell:
#         """
#         pairtools select '(\
#                           (pair_type.upper()=="UR" or \
#                            pair_type.upper()=="UU" or \
#                            pair_type.upper()=="RU") and \
#                           (chrom2=="{params.selection_chrom}") and \
#                           (pos2>{params.selection_start}) and \
#                           (pos2<{params.selection_end}))' \
#         -o {output} {input} \
#         >{log[0]} 2>&1
#         """

rule coverage:
    input:
        pairs=os.path.join(output_dir, "pairs/{sample}_{side}.pairs"),
        script='workflow/scripts/coverage.py'
    output:
        bg=os.path.join(output_dir, "coverage/{sample}_{side}_coverage.bedgraph"),
        bw=os.path.join(output_dir, "coverage/{sample}_{side}_coverage.bw"),
    conda:
        "envs/pairtools_env.yaml",
    log:
        "logs/coverage/{sample}_{side}.log",
    benchmark:
        "benchmarks/coverage/{sample}_{side}.tsv"
    threads: 1
    params:
        side=1
    shell:
        """
        python3 {input.script} -i {input.pairs} --side {params.side} -o {output.bg} \
        -t {threads} --output-bigwig {output.bw} \
        >{log[0]} 2>&1
        """


rule for_ucsc:
    input:
        bg=os.path.join(output_dir, "coverage/{sample}_{side}_coverage.bedgraph"),
        script='workflow/scripts/for_ucsc.py'
    output:
        bg=os.path.join(output_dir, "coverage/{sample}_{side}_coverage_for_ucsc.bedgraph"),
        bw=os.path.join(output_dir, "coverage/{sample}_{side}_coverage_for_ucsc.bw"),
    conda:
        "envs/pairtools_env.yaml",
    params:
        chromsizes=config['chrom_sizes_path_no_cassette'],
        name=lambda wildcards: "{wildcards.sample}_{wildcards.side}"
    shell:
        """
        python3 {input.script} -i {input.bg} --chromsizes {params.chromsizes} \
        --name {params.name} -o {output.bg} --output-bigwig {output.bw}
        """

rule find_peaks:
    input:
        coverage=os.path.join(output_dir, "coverage/{sample}_{side}_coverage.bedgraph"),
        script='workflow/scripts/find_peaks.py'
    output:
        os.path.join(output_dir, "peaks/{sample}_{side}.bed")
    conda:
        "envs/peaks.yaml",
    log:
        "logs/find_peaks/{sample}_{side}.log",
    benchmark:
        "benchmarks/find_peaks/{sample}_{side}.tsv"
    threads: 1
    params:
        cluster_arg = lambda wildcards, input: "--no-cluster" if config['use_only_read_junctions'] else "--cluster",
        auto_li_arg = lambda wildcards, input: "--auto-li" if config['use_only_read_junctions'] else "--no-auto-li",
        min_peak_width=config['min_peak_width'],
        min_peak_reads=config['min_peak_reads'],
        min_peak_frac=config['min_peak_frac'],
        min_peak_dist=config['min_peak_dist'],
        cassette_name=config['cassette_name'],
        ignore_chrom_arg=''
    shell:
        """
        python3 {input.script} -i {input.coverage} -o {output} \
        {params.cluster_arg} {params.auto_li_arg} \
        --min-peak-width {params.min_peak_width} --min-peak-frac {params.min_peak_frac} \
        --min-peak-reads {params.min_peak_reads} --ignore-chrom {params.cassette_name} \
        {params.ignore_chrom_arg} \
        >{log[0]} 2>&1
        """

rule combine_peaks:
    input:
        peaks_fwd=os.path.join(output_dir, "peaks/{sample}_forward.bed"),
        peaks_rev=os.path.join(output_dir, "peaks/{sample}_reverse.bed"),
        blacklist=config.get('blacklist', []),
        script='workflow/scripts/combine_peaks.py'
    output:
        os.path.join(output_dir, "peaks/{sample}_peaks.bed"),
    conda:
        "envs/peaks.yaml",
    log:
        "logs/combine_peaks/{sample}.log",
    benchmark:
        "benchmarks/combine_peaks/{sample}.tsv"
    threads: 1
    params:
        blacklist_arg = lambda wildcards, input: "--blacklist {input.blacklist}" if input.blacklist else "",
    shell:
        """
        python3 {input.script} --fwd {input.peaks_fwd} --rev {input.peaks_rev} \
        {params.blacklist_arg} \
        --sample-name {wildcards.sample} -o {output} \
        >{log[0]} 2>&1
        """

rule combine_all_peaks:
    input:
        peaks=expand(os.path.join(output_dir, "peaks/{sample}_peaks.bed"),
                            sample=sample_list)
    output:
        os.path.join(output_dir, "peaks/all_peaks.bed"),
    conda:
        "envs/peaks.yaml",
    log:
        "logs/combine_all_peaks/log.log",
    benchmark:
        "benchmarks/combine_all_peaks/benchmark.tsv"
    threads: 1
    shell:
        """
        cat {input.peaks} | cut -f1,2,3,4,6,7 | sort -k4,4V -k1,1V -k2,2n -k3,3n -k5,5 > {output}
        """

rule find_insertion_sites:
    input:
        peaks=os.path.join(output_dir, "peaks/all_peaks.bed"),
        script='workflow/scripts/find_insertion_sites.py' #lambda wildcards: 'workflow/scripts/find_insertion_sites_readthrough.py' if config['use_junction_readthrough'] else 'workflow/scripts/find_insertion_sites.py'
    output:
        os.path.join(output_dir, "insertion_sites/all_sites.bed"),
    conda:
        "envs/peaks.yaml",
    log:
        "logs/find_insertion_sites/all.log",
    benchmark:
        "benchmarks/find_insertion_sites/all.tsv"
    threads: 1
    params:
        refgen_path=refgen_path,
        insertion_seq=config['insertion_seq']
    shell:
        """
        python3 {input.script} --peaks {input.peaks} \
        --genome {params.refgen_path} --insertion-seq {params.insertion_seq} \
        -o {output} \
        >{log[0]} 2>&1
        """